# ğŸ” SecureSense â€” Sensitive Data Detection using Transformers

SecureSense is a high-performance NLP system designed to automatically detect sensitive information in text using a fine-tuned Transformer model.

Built with a production-oriented mindset, this project demonstrates an end-to-end machine learning pipeline â€” from dataset preparation and token alignment to GPU training, evaluation, and inference readiness.

---

## ğŸš€ Project Overview

Sensitive data detection is critical for:

- Privacy protection
- Regulatory compliance (GDPR, HIPAA)
- Secure data pipelines
- Enterprise document processing

SecureSense leverages **DistilBERT** for token-level classification to identify personally identifiable information (PII) with strong precision-recall balance.

---

## ğŸ§  Model Architecture

- **Base Model:** DistilBERT (`distilbert-base-uncased`)
- **Task:** Named Entity Recognition (Token Classification)
- **Framework:** HuggingFace Transformers
- **Training Device:** NVIDIA GTX 1650 GPU (FP16 enabled)

### Key Training Strategies:

- Linear learning rate scheduler with warmup
- Gradient accumulation for memory efficiency
- Best-model checkpointing
- Automated evaluation per epoch

---

## ğŸ“Š Performance

| Metric       | Score     |
| ------------ | --------- |
| **F1 Score** | **0.818** |
| Precision    | 0.808     |
| Recall       | 0.828     |
| Eval Loss    | 0.278     |

### âœ” Interpretation:

- Strong generalization
- Stable convergence
- Balanced precision-recall
- Recall slightly prioritized â€” desirable for sensitive data detection systems

---

## ğŸ—ï¸ Pipeline Architecture

```
Dataset â†’ Tokenization â†’ Label Alignment â†’ Transformer Fine-Tuning â†’ Evaluation â†’ Metrics Export
```

### Training Pipeline Includes:

âœ… Batched tokenization  
âœ… Word-piece label alignment  
âœ… GPU acceleration  
âœ… Experiment logging  
âœ… Automatic best-model selection

---

## ğŸ“‚ Project Structure

```
SecureSense/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py              # End-to-end training pipeline
â”‚   â”œâ”€â”€ preprocess.py        # Tokenization + label alignment
â”‚   â”œâ”€â”€ data_loader.py       # Dataset loading
â”‚   â”œâ”€â”€ metrics.py           # Precision / Recall / F1 computation
â”‚   â”œâ”€â”€ predict.py           # Inference utilities
â”‚   â”œâ”€â”€ config.py           # Central configuration
â”‚   â””â”€â”€ analyse_results.py   # Training visualization
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py              # FastAPI scaffold for deployment
â”‚
â”œâ”€â”€ Dockerfile              # Container-ready setup
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/YOUR_USERNAME/SecureSense.git
cd SecureSense

python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

---

## ğŸ‹ï¸ Training

```bash
python src/train.py
```

Training features:

- Automatic evaluation
- Metrics saved to `/results`
- Best checkpoint retained
- GPU optimized

---

## ğŸ” Example Use Case

Input:

```
"My name is John and I live in New York."
```

Output:

```
John â†’ PERSON
New York â†’ LOCATION
```

---

## ğŸ“ˆ Experiment Tracking

Training logs and metrics are exported as CSV files for analysis and reproducibility.

Supports visualization of:

- Loss curves
- F1 progression
- Precision vs Recall

---

## ğŸ§ª Engineering Highlights

This project emphasizes **real-world ML engineering practices**, including:

- Modular code design
- Config-driven pipeline
- GPU-aware training
- Memory-efficient batching
- Structured evaluation
- Deployment-ready API scaffold

---

## ğŸ”® Future Enhancements

Planned improvements include:

- Hybrid detection (Transformer + rule-based patterns)
- Automatic PII masking/redaction
- Production API deployment
- Real-time inference
- Expanded dataset training

---

## â­ Why This Project Matters

SecureSense demonstrates the ability to:

âœ” Train transformer models effectively  
âœ” Optimize under hardware constraints  
âœ” Evaluate using correct NLP metrics  
âœ” Structure production-style ML codebases

This is not a notebook experiment â€” it is a system-oriented implementation of modern NLP practices.

---
